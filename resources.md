#List of the resouces:
#https://github.com/google-research/google-research/tree/master/memory_efficient_attention
#https://arxiv.org/pdf/2210.04995.pdf
#https://arxiv.org/pdf/2403.07816.pdf
#https://arxiv.org/html/2402.12656v1
#https://arxiv.org/pdf/2312.09877.pdf


### Invertible MLP:
https://openreview.net/pdf?id=-jnE7sxuMm
#### the invertible activation function is leaky relu




#https://jmlr.org/papers/volume22/19-1028/19-1028.pdf
#https://arxiv.org/pdf/1908.09257.pdf
#https://towardsdatascience.com/introduction-to-normalizing-flows-d002af262a4b
#https://deepgenerativemodels.github.io/notes/flow/
#https://ankurdhuriya.medium.com/what-are-normalizing-flows-ce7ccd222ee7
#https://paperswithcode.com/method/normalizing-flows#:~:text=Normalizing%20Flows%20are%20a%20method,the%20sequence%20of%20invertible%20mappings.
